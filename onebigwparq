config.json

{
    "data_source": "parquet",
    "server": "<your_server>.database.windows.net",
    "database": "<your_database>",
    "username": "<your_user>@<your_tenant>.com",
    "password": "<your_password>",
    "table": "YourTableName",
    "parquet_dir": "parquet_chunks",
    "parquet_chunk_days": 1,
    "output_dir": "alerts",
    "business_rules_file": "config/business_rules.json"
}

--------------------------------------------------------
parquet_downloader.py

import pyodbc
import pandas as pd
import os
import json
import argparse

def get_db_config(config_path="config/config.json"):
    with open(config_path, "r") as f:
        return json.load(f)

def download_to_parquet_chunks(
    server, database, username, password,
    table,
    start_date, end_date,
    chunk_days=1,
    out_dir="parquet_chunks"
):
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    conn_str = (
        f"DRIVER={{ODBC Driver 17 for SQL Server}};"
        f"SERVER={server};"
        f"DATABASE={database};"
        f"UID={username};"
        f"PWD={password};"
        "Authentication=ActiveDirectoryPassword;"
        "Encrypt=yes;TrustServerCertificate=no;"
        "Connection Timeout=30;"
        "Command Timeout=300;"
    )
    conn = pyodbc.connect(conn_str, autocommit=True)
    date_range = pd.date_range(start=start_date, end=end_date, freq=f"{chunk_days}D")
    total_rows = 0
    for i, chunk_start in enumerate(date_range):
        chunk_end = min(chunk_start + pd.Timedelta(days=chunk_days-1), pd.to_datetime(end_date))
        query = f"""
        SELECT * FROM {table} WITH (NOLOCK)
        WHERE [Date] >= '{chunk_start.date()}' AND [Date] <= '{chunk_end.date()}'
        """
        df = pd.read_sql(query, conn)
        if not df.empty:
            file_name = f"{out_dir}/cq_chunk_{chunk_start.date()}_{chunk_end.date()}.parquet"
            df.to_parquet(file_name, index=False)
            print(f"✅ Saved {len(df):,} rows to {file_name}")
            total_rows += len(df)
    conn.close()
    print(f"✅ All done. Total rows: {total_rows:,}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Download SQL data in parquet chunks")
    parser.add_argument('--start_date', required=True, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end_date', required=True, help='End date (YYYY-MM-DD)')
    args = parser.parse_args()

    config = get_db_config()
    download_to_parquet_chunks(
        config["server"],
        config["database"],
        config["username"],
        config["password"],
        table=config.get("table", "YourTableName"),
        start_date=args.start_date,
        end_date=args.end_date,
        chunk_days=config.get("parquet_chunk_days", 1),
        out_dir=config.get("parquet_dir", "parquet_chunks")
    )
----------------------------------------------------------------------------

main.py

import argparse
import json
import os
import glob
import pandas as pd
from datetime import datetime
from db.db_connection import get_engine, fetch_data
from features.feature_engineering import clean_and_engineer
from models.anomaly_detection import detect_anomalies
from models.xgboost_classifier import train_xgboost_classifier
from alerts.alert_generator import generate_detailed_alerts, generate_cluster_alerts

def extract_dates_from_filename(filename):
    # Expects files like cq_chunk_YYYY-MM-DD_YYYY-MM-DD.parquet
    base = os.path.basename(filename)
    parts = base.replace(".parquet", "").split("_")
    if len(parts) < 4:
        return None, None
    try:
        date1 = datetime.strptime(parts[2], "%Y-%m-%d").date()
        date2 = datetime.strptime(parts[3], "%Y-%m-%d").date()
        return date1, date2
    except Exception:
        return None, None

def load_data_from_parquet(parquet_dir, start_date=None, end_date=None):
    files = glob.glob(os.path.join(parquet_dir, "*.parquet"))
    if start_date and end_date:
        s_date = datetime.strptime(start_date, "%Y-%m-%d").date()
        e_date = datetime.strptime(end_date, "%Y-%m-%d").date()
        filtered_files = []
        for f in files:
            f_start, f_end = extract_dates_from_filename(f)
            if f_start and f_end:
                # File overlaps desired range if either end is within range
                if (f_start <= e_date and f_end >= s_date):
                    filtered_files.append(f)
        files = filtered_files
    if not files:
        raise ValueError(f"No parquet files found in {parquet_dir} for the given date range.")
    dfs = [pd.read_parquet(f) for f in sorted(files)]
    df = pd.concat(dfs, ignore_index=True)
    print(f"✅ Loaded {len(df):,} rows from {len(files)} parquet files.")
    return df

def main():
    parser = argparse.ArgumentParser(description="Teams CQD Anomaly Detection")
    parser.add_argument('--start_date', required=False, help='Start Date (YYYY-MM-DD)')
    parser.add_argument('--end_date', required=False, help='End Date (YYYY-MM-DD)')
    parser.add_argument('--output_dir', help='Output Directory')
    args = parser.parse_args()

    with open('config/config.json', 'r') as f:
        config = json.load(f)

    output_dir = args.output_dir or config.get('output_dir', 'alerts')
    os.makedirs(output_dir, exist_ok=True)

    data_source = config.get('data_source', 'sql')
    if data_source == "parquet":
        parquet_dir = config.get("parquet_dir", "parquet_chunks")
        df = load_data_from_parquet(
            parquet_dir,
            start_date=args.start_date,
            end_date=args.end_date
        )
    else:
        engine = get_engine(
            config["server"],
            config["database"],
            config["username"],
            config["password"]
        )
        print("✅ Database connection successful!")
        df = fetch_data(engine, args.start_date, args.end_date)

    with open(config.get("business_rules_file", "config/business_rules.json"), 'r') as f:
        business_rules = json.load(f)

    df = clean_and_engineer(df, business_rules)

    train_xgboost_classifier(df)

    anomalies = detect_anomalies(df, business_rules)

    generate_detailed_alerts(anomalies, business_rules, output_dir, args.start_date, args.end_date)
    generate_cluster_alerts(anomalies, business_rules, output_dir, args.start_date, args.end_date)
    print("Alerts generated at", output_dir)

if __name__ == "__main__":
    main()
